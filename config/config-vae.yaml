# VAE Configuration

# Resume training from checkpoint (optional)
resume: null 

data:
  train_dir: "dataset/vae_dataset_pc/png_scaled/train"
  valid_dir: "dataset/vae_dataset_pc/png_scaled/validation"
  archetypes_dir: "dataset/archetypes_pc/png_scaled"  # Labeled wireframes for latent metrics
  batch_size: 4                    # SmartBatchSampler groups similar-sized images to minimize padding
  num_workers: 4                   # I/O parallelization (reduced for RAM)
  seed: 42                          # Reproducibility
  
  # Memory management
  max_height: 1500                  # Maximum height (crop if exceeded)
  
  # Denoising augmentation
  noise_level: 0.20                 # Gaussian noise level for robustness
  
  # Data augmentation (applied to training only)
  augment: true
  sp_prob: 0.04                     # Salt-and-pepper noise probability
  random_erasing_prob: 0.35         # Random masking probability
  perspective_p: 0.3                # Perspective transform probability
  perspective_distortion_scale: 0.12  # Perspective distortion magnitude
  
  # Additional augmentations
  rotation_degrees: 2               # Random rotation range
  brightness_jitter: 0.1            # Brightness variation
  contrast_jitter: 0.1              # Contrast variation

# Model architecture
model:
  name: "vae"
  class: "VAE"                   # Must match class in vae_models.py
  latent_dim: 128                # Latent space dimensionality
  input_channels: 1              # Grayscale images
  dropout_p: 0.1                 # Dropout in decoder
  # use_skip_connections: false  # Skip connections disabled (can cause posterior collapse in VAE)

# Loss configuration
# Options: 'l1', 'mse', 'perceptual'
# 'perceptual' = L1 + SSIM + Gradient + Multi-scale (recommended for wireframes)
#
# Standard VAE normalization:
# - recon_loss: SUM over all pixels (~50,000 for 1M pixels)
# - kld_loss: SUM over dims, MEAN over batch (~20-50 typically)
#
# With sum reduction: reconstruction naturally dominates by ~1000x
# Therefore beta_kld = 1.0 restores proper balance without posterior collapse
loss:
  name: "l1"                     # 'l1', 'mse', or 'perceptual'
  beta_kld: 1                  # Using sum reduction: naturally balanced at 1.0
  loss_scale: 1                  # No scaling (natural values for debugging)
  warmup_epochs: 0               # KLD warmup epochs (prevents early posterior collapse)
  
  # Component weights (only used when name='perceptual')
  lambda_l1: 1.0                 # Pixel accuracy
  lambda_ssim: 0.5               # Structural similarity
  lambda_gradient: 0.1           # Edge preservation
  lambda_multiscale: 0.2         # Multi-scale hierarchical features
  use_multiscale: true           # Enable multi-scale loss

# Optimizer configuration
optim:
  algo: "AdamW"
  params:
    lr: 0.0003                   # Initial learning rate
    weight_decay: 0.0001         # L2 regularization
  
  # Learning rate scheduler
  scheduler:
    name: "CosineAnnealingWarmRestarts"
    params:
      T_0: 10                    # Restart every 10 epochs
      T_mult: 2                  # Double period after each restart
      eta_min: 0.000003          # Minimum learning rate
      verbose: true

# Training optimization
# Note: lr and weight_decay are defined in optim.params
optimization:
  accumulation_steps: 4          # Gradient accumulation (effective batch = batch_size * accumulation_steps = 16)
  mixed_precision: true          # Automatic mixed precision for speed and memory

# Logging configuration
logging:
  logdir: "./logs"
  tensorboard: true
  log_loss_components: true      # Log SSIM/gradient/KLD separately
  save_reconstructions_every: 5
  latent_visualization:
    frequency: 20                # t-SNE visualization every 20 epochs
    max_samples: 500
  # wandb:
  #   project: "ux-vae-improved"
  #   entity: "your-team"

# Training configuration
nepochs: 100

# Early stopping
early_stopping:
  enabled: true
  monitor: "test_SSIM"           # Monitored metric (or "test_ELBO")
  mode: "max"                    # 'max' for SSIM, 'min' for ELBO
  patience: 20                   # Stop if no improvement for 20 epochs
  min_delta: 0.0001              # Minimum improvement threshold
# Test (inference) configuration
# Usage: python -m src.torchtmpl.main config/config-vae.yaml test
test:
  test_input_dir: "dataset/vae_dataset_pc/validation_scaled"          # Directory containing test images
  test_output_dir: "test/test_output"        # Output directory for comparison grids
  model_path: "logs/VAE_5/last_model.pt"     # Path to trained model checkpoint

# Interpolation configuration
# Usage: python -m src.torchtmpl.main config/config-vae.yaml interpolate
interpolate:
  image1_path: "test/test_input/wireframe_2983236_linear.png"  # First image path
  image2_path: "test/test_input/wireframe_3008792_linear.png"  # Second image path
  output_dir: "test/interpolate_output"            # Output directory for interpolation grid
  num_steps: 10                                     # Number of interpolation steps
  model_path: "logs/VAE_0/last_model.pt"           # Path to trained model checkpoint

# Clustering configuration
# Usage: python -m src.torchtmpl.main config/config-vae.yaml clustering
clustering:
  model_path: "logs/VAE_2/last_model.pt"           # Path to trained model checkpoint
  data_dir: "dataset/vae_dataset_pc/png_scaled/validation"  # Directory containing images to cluster
  output_dir: null                                  # Output directory (null = same as model directory)
  max_samples: 4000                                 # Maximum number of samples to process
  n_clusters: 15                                    # Number of clusters for k-means
  viz_method: "both"                                # Visualization method: 'pca', 'tsne', or 'both'
  archetypes_dir: "dataset/archetypes_pc/png"      # Optional: archetypes for reference