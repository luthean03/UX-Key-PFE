data:
  data_dir: "/usr/users/sdim/sdim_31/UX-Key-PFE/vae_dataset" # Le dossier généré par ton script précédent
  batch_size: 1 # Batch size 1 pour tailles variables et SPP
  num_workers: 8
  valid_ratio: 0.2 # Fraction pour validation (train/valid split)
  # === NOUVEAU : Réglage du bruit pour le Denoising ===
  noise_level: 0.15    # Quantité de bruit gaussien (0.0 à 1.0)
  max_height: 2048
  # === AUGMENTATIONS (Web-safe) ===
  augment: true                   # Activer/désactiver les augmentations pour le train
  sp_prob: 0.02                   # Probabilité (par pixel) de salt-and-pepper
  random_erasing_prob: 0.2       # Probabilité d'appliquer RandomErasing sur l'input
  perspective_p: 0.3              # Probabilité d'appliquer RandomPerspective
  perspective_distortion_scale: 0.08

model:
  name: "vae"
  class: "VAE"  # <--- INDISPENSABLE : Doit correspondre au nom de la classe dans vae_models.py
  latent_dim: 128
  input_channels: 1

# === NOUVELLE SECTION LOSS ===
loss:
  name: "perceptual"         # Options: "l1", "mse", "perceptual"
  perceptual_weight: 0.01    # Poids de la composante VGG (0.02-0.1 typiquement)
  beta_kld: 0.001    # Poids de la régularisation KL
  # KL Annealing : Nombre d'époques pour passer de Beta=0 à Beta_target
  # Mettez 0 pour désactiver l'annealing.
  # Mettez 20 pour une chauffe progressive (recommandé pour la stabilité).
  warmup_epochs: 20

optim:
  algo: "Adam"
  params:
    lr: 0.0005 # Les VAE aiment les learning rates bas
    weight_decay: 0.00001
  epochs: 100
  beta_kld: 0.001 # Poids de la perte KL (commence bas)

  # === Scheduler (LR) ===
  scheduler:
    name: "ReduceLROnPlateau"
    params:
      mode: "min"         # On veut minimiser la loss
      factor: 0.5         # On divise le LR par 2 quand ça stagne
      patience: 5         # Nb d'époques sans amélioration
      verbose: True       # Affiche quand le LR change
      min_lr: 0.00001     # Plancher

# === OPTIMIZATION (Gradient accumulation configurable) ===
optimization:
  lr: 0.0005
  weight_decay: 0.00001
  # Définit combien d'images on traite avant de mettre à jour les poids.
  # Comme `batch_size` réel est souvent 1, ceci définit votre batch virtuel.
  accumulation_steps: 128

# Logging / monitoring (minimal). The trainer expects a `logging` section.
logging:
  logdir: "./logs"
  # Enable TensorBoard image + scalar logging
  tensorboard: True

# Nombre d'epochs pour la boucle principale (compatibilité avec main.train)
nepochs: 100